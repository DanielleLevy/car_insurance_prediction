{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Bank Marketing Dataset\n",
    "\n",
    "The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\n",
    "\n",
    "It is commonly used for fairness tasks and marital status is often used as the sensitive attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "import fairness_functions as fp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fetch dataset \n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "bank_marketing = fetch_ucirepo(id=222) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = bank_marketing.data.features \n",
    "y = bank_marketing.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(bank_marketing.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(bank_marketing.variables) \n",
    "\n",
    "# define middle age as sensitive attribute\n",
    "X['middle_aged'] = X['age'].apply(lambda x: 1 if 25 <= x <= 60 else 0)\n",
    "\n",
    "sensitive_col ='middle_aged'\n",
    "\n",
    "X = X.dropna(subset=[sensitive_col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Nan Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputes numeric Nan values with column mean and Nans in categorical columns with column mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns based on dataset description\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', \n",
    "                    'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "# Define numeric columns by excluding categorical and target columns\n",
    "numeric_cols = [col for col in X.columns if col not in categorical_cols + ['y']]\n",
    "\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "# Convert numeric columns to numeric dtype (forcing non-numeric values to NaN)\n",
    "X_numeric = X[numeric_cols].apply(lambda col: pd.to_numeric(col, errors='coerce'))\n",
    "\n",
    "# Fill missing values in numeric columns with the mean of each column.\n",
    "X_numeric = X_numeric.fillna(X_numeric.mean())\n",
    "\n",
    "# For categorical columns, filter out any with high cardinality (e.g., >20 unique values)\n",
    "max_unique_threshold = 20\n",
    "filtered_categorical_cols = [col for col in categorical_cols if X[col].nunique() <= max_unique_threshold]\n",
    "print(\"Filtered Categorical columns (<=20 unique values):\", filtered_categorical_cols)\n",
    "\n",
    "# Process categorical columns: fill missing values with the mode.\n",
    "X_categorical = X[filtered_categorical_cols].copy()\n",
    "for col in filtered_categorical_cols:\n",
    "    X_categorical[col] = X_categorical[col].fillna(X_categorical[col].mode()[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encode the filtered categorical columns using pandas' get_dummies, dropping the first category.\n",
    "X_categorical_encoded = pd.get_dummies(X_categorical, drop_first=True)\n",
    "\n",
    "# Combine numeric and one-hot encoded categorical columns.\n",
    "X_processed = pd.concat([X_numeric, X_categorical_encoded], axis=1)\n",
    "\n",
    "# Fill any remaining NaN values with 0.\n",
    "X_processed = X_processed.fillna(0)\n",
    "\n",
    "# Preserve the sensitive attribute for fairness evaluation.\n",
    "sens = X[sensitive_col]\n",
    "\n",
    "print(\"Shape of processed features:\", X_processed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = y.squeeze()  # Converts a single-column DataFrame to a Series if needed\n",
    "\n",
    "# Convert categorical target labels to binary\n",
    "y = y.map({'yes': 1, 'no': 0})\n",
    "\n",
    "\n",
    "# Split data and also split the sensitive attribute for evaluation\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X_processed, y, sens, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "print(\"X train shape: \",X_train.shape)\n",
    "print(\"X test shape: \",X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model\n",
    "lr = LogisticRegression(random_state=42, max_iter=10000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set with the baseline model\n",
    "y_pred_baseline = lr.predict(X_test)\n",
    "\n",
    "# Evaluate baseline performance metrics\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "f1_score_baseline = f1_score(y_test, y_pred_baseline)\n",
    "\n",
    "# Evaluate fairness metrics for the baseline model\n",
    "baseline_dp_diff = demographic_parity_difference(y_test, y_pred_baseline, sensitive_features=sens_test)\n",
    "baseline_eo_diff = equalized_odds_difference(y_test, y_pred_baseline, sensitive_features=sens_test)\n",
    "\n",
    "print(\"=== Baseline Model Metrics ===\")\n",
    "print(\"Accuracy:\", baseline_accuracy)\n",
    "print(\"F1 score:\",f1_score_baseline) \n",
    "print(\"Demographic Parity Difference:\", baseline_dp_diff)\n",
    "print(\"Equalized Odds Difference:\", baseline_eo_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive solution - drop sensitive column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process X_processed as before\n",
    "# Drop sensitive columns from the entire processed dataset\n",
    "X_processed_no_sensitive = X_processed.drop(columns=sensitive_col)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X_processed_no_sensitive, y, sens, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train the logistic regression model\n",
    "lr = LogisticRegression(random_state=42,max_iter=10000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_naive = lr.predict(X_test)\n",
    "\n",
    "# Evaluate baseline performance metrics\n",
    "naive_accuracy = accuracy_score(y_test, y_pred_naive)\n",
    "f1_score_naive = f1_score(y_test, y_pred_naive)\n",
    "\n",
    "# Evaluate fairness metrics for the baseline model\n",
    "naive_dp_diff = demographic_parity_difference(y_test, y_pred_naive, sensitive_features=sens_test)\n",
    "naive_eo_diff = equalized_odds_difference(y_test, y_pred_naive, sensitive_features=sens_test)\n",
    "\n",
    "print(\"=== Naive Model Metrics ===\")\n",
    "print(\"Accuracy:\", naive_accuracy)\n",
    "print(\"F1 score:\",f1_score_naive) \n",
    "print(\"Demographic Parity Difference:\", naive_dp_diff)\n",
    "print(\"Equalized Odds Difference:\", naive_eo_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal fairness search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define candidate methods for each stage.\n",
    "pre_methods = {\n",
    "    \"None\": fp.pre_none,\n",
    "    \"CorrRemover\": fp.pre_correlation_remover,\n",
    "    \"SensitiveResampling\": fp.pre_sensitive_resampling  # new candidate\n",
    "}\n",
    "\n",
    "in_methods = {\n",
    "    \"Baseline\": fp.in_baseline,\n",
    "    \"Reweighting\": fp.in_reweighting,\n",
    "    \"ExpGrad_DP\": fp.in_expgrad_dp,\n",
    "    \"ExpGrad_EO\": fp.in_expgrad_eo\n",
    "}\n",
    "\n",
    "post_methods = {\n",
    "    \"None\": fp.post_none,\n",
    "    \"Threshold_DP\": fp.post_threshold_dp,\n",
    "    \"Threshold_EO\": fp.post_threshold_eo\n",
    "}\n",
    "\n",
    "# Run experiments:\n",
    "results = fp.run_experiments(pre_methods, in_methods, post_methods,\n",
    "                             X_train, y_train, sens_train,\n",
    "                             X_test, y_test, sens_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only pareto optimal methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "objectives = {\"f1_score\": True, \"Demographic_parity\": False, \"Equalized_odds\": False}\n",
    "\n",
    "frontier = fp.pareto_frontier(results, objectives)\n",
    "\n",
    "print(\"Pareto Frontier configurations:\")\n",
    "for config, metrics in frontier.items():\n",
    "    print(f\"{config}: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply thresholds on biase and portion of retained accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set thresholds on accurcy, demographic parity and equalized odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_threshold = 0.38\n",
    "demographic_parity_threshold = 0.1\n",
    "equalized_odds_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results based on thresholds.\n",
    "filtered = fp.filter_results(frontier, f1_threshold=f1_threshold,\n",
    "                            dp_threshold=demographic_parity_threshold, eo_threshold=equalized_odds_threshold)\n",
    "\n",
    "print(\"\\nFiltered Results (satisfying thresholds):\")\n",
    "for config, metrics in filtered.items():\n",
    "    print(config, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
