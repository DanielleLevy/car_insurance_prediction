{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - German Credit Dataset\n",
    "\n",
    "The German Credit Dataset classifies people described by a set of attributes as good or bad credit risks. \n",
    "\n",
    "It is commonly used for fairness tasks and  \"personal_status_sex\", which combined gender and marital status, is usually the protected attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "import fairness_functions as fp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "  \n",
    "# Fetch dataset\n",
    "statlog_german_credit_data = fetch_ucirepo(id=144)\n",
    "  \n",
    "# Data (as pandas dataframes)\n",
    "X = statlog_german_credit_data.data.features\n",
    "y = statlog_german_credit_data.data.targets\n",
    "  \n",
    "\n",
    "# Manually define column names based on the dataset's variable information.\n",
    "# Here is an example list of column names:\n",
    "feature_col_names = [\n",
    "    \"checking_account_status\",    # Attribute1: Status of existing checking account\n",
    "    \"duration\",                   # Attribute2: Duration (months)\n",
    "    \"credit_history\",             # Attribute3: Credit history\n",
    "    \"purpose\",                    # Attribute4: Purpose\n",
    "    \"credit_amount\",              # Attribute5: Credit amount\n",
    "    \"savings_account_bonds\",      # Attribute6: Savings account/bonds\n",
    "    \"present_employment_since\",   # Attribute7: Present employment since (Other)\n",
    "    \"installment_rate\",           # Attribute8: Installment rate in percentage of disposable income\n",
    "    \"personal_status_sex\",        # Attribute9: Marital Status / Personal status and sex\n",
    "    \"other_debtors\",              # Attribute10: Other debtors / guarantors\n",
    "    \"present_residence_since\",    # Attribute11: Present residence since\n",
    "    \"property\",                   # Attribute12: Property\n",
    "    \"age\",                        # Attribute13: Age (years)\n",
    "    \"other_installment_plans\",    # Attribute14: Other installment plans\n",
    "    \"housing\",                    # Attribute15: Housing (Other)\n",
    "    \"number_of_existing_credits\", # Attribute16: Number of existing credits at this bank\n",
    "    \"occupation\",                 # Attribute17: Occupation (Job)\n",
    "    \"number_of_people_liable\",    # Attribute18: Number of people being liable to provide maintenance for\n",
    "    \"telephone\",                  # Attribute19: Telephone (Binary)\n",
    "    \"foreign_worker\"              # Attribute20: foreign worker (Binary)\n",
    "]\n",
    "\n",
    "# Assign these column names to the features DataFrame if not already set.\n",
    "X.columns = feature_col_names\n",
    "\n",
    "sensitive_col ='personal_status_sex'\n",
    "\n",
    "X = X.dropna(subset=[sensitive_col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust target column to be binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(y, pd.DataFrame):\n",
    "    y = y.squeeze()\n",
    "\n",
    "y = y.map({1: 0, 2: 1})\n",
    "print(\"Unique target values after mapping:\", y.unique())\n",
    "\n",
    "print(y.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Nan Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputes numeric Nan values with column mean and Nans in categorical columns with column mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which columns are categorical based on domain knowledge for German Credit data.\n",
    "categorical_cols = [\n",
    "    \"checking_account_status\",  # e.g., categorical status of checking account\n",
    "    \"credit_history\",           # credit history categories\n",
    "    \"purpose\",                  # purpose of credit\n",
    "    \"savings_account_bonds\",    # savings account/bonds categories\n",
    "    \"present_employment_since\", # employment status (categorical)\n",
    "    \"personal_status_sex\",      # combined personal status and sex\n",
    "    \"other_debtors\",            # categorical: other debtors/guarantors\n",
    "    \"property\",                 # property information (categorical)\n",
    "    \"other_installment_plans\",  # other installment plans\n",
    "    \"housing\",                  # housing situation (categorical)\n",
    "    \"occupation\",               # occupation categories\n",
    "    \"telephone\",                # binary, but treated as categorical\n",
    "    \"foreign_worker\"            # binary, but treated as categorical\n",
    "]\n",
    "\n",
    "# All remaining columns are considered numeric.\n",
    "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "# Convert numeric columns to numeric dtype (forcing non-numeric values to NaN)\n",
    "X_numeric = X[numeric_cols].apply(lambda col: pd.to_numeric(col, errors='coerce'))\n",
    "\n",
    "# Fill missing values in numeric columns with the mean of each column.\n",
    "X_numeric = X_numeric.fillna(X_numeric.mean())\n",
    "\n",
    "# Filter the categorical columns: drop any that have high cardinality (threshold = 20 unique values)\n",
    "max_unique_threshold = 20\n",
    "filtered_categorical_cols = [col for col in categorical_cols if X[col].nunique() <= max_unique_threshold]\n",
    "print(\"Filtered Categorical columns (<=20 unique values):\", filtered_categorical_cols)\n",
    "\n",
    "# Process the categorical columns: fill missing values with the mode.\n",
    "X_categorical = X[filtered_categorical_cols].copy()\n",
    "for col in filtered_categorical_cols:\n",
    "    X_categorical[col] = X_categorical[col].fillna(X_categorical[col].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encode the filtered categorical columns using pandas' get_dummies, dropping the first category.\n",
    "X_categorical_encoded = pd.get_dummies(X_categorical, drop_first=True)\n",
    "\n",
    "# Combine numeric and one-hot encoded categorical columns.\n",
    "X_processed = pd.concat([X_numeric, X_categorical_encoded], axis=1)\n",
    "\n",
    "# Fill any remaining NaN values with 0.\n",
    "X_processed = X_processed.fillna(0)\n",
    "\n",
    "# Preserve the sensitive attribute for fairness evaluation.\n",
    "sens = X[sensitive_col]\n",
    "\n",
    "print(\"Shape of processed features:\", X_processed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data to train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and also split the sensitive attribute for evaluation\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X_processed, y, sens, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "print(\"X train shape: \",X_train.shape)\n",
    "print(\"X test shape: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model\n",
    "lr = LogisticRegression(random_state=42, max_iter=10000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set with the baseline model\n",
    "y_pred_baseline = lr.predict(X_test)\n",
    "\n",
    "# Evaluate baseline performance metrics\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "f1_score_baseline = f1_score(y_test, y_pred_baseline)\n",
    "\n",
    "# Evaluate fairness metrics for the baseline model\n",
    "baseline_dp_diff = demographic_parity_difference(y_test, y_pred_baseline, sensitive_features=sens_test)\n",
    "baseline_eo_diff = equalized_odds_difference(y_test, y_pred_baseline, sensitive_features=sens_test)\n",
    "\n",
    "print(\"=== Baseline Model Metrics ===\")\n",
    "print(\"Accuracy:\", baseline_accuracy)\n",
    "print(\"F1 score:\",f1_score_baseline) \n",
    "print(\"Demographic Parity Difference:\", baseline_dp_diff)\n",
    "print(\"Equalized Odds Difference:\", baseline_eo_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive solution - drop sensitive column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process X_processed as before\n",
    "# Drop sensitive columns from the entire processed dataset\n",
    "sensitive_encoded_cols = [col for col in X_processed.columns if col.startswith(sensitive_col + '_')]\n",
    "X_processed_no_sensitive = X_processed.drop(columns=sensitive_encoded_cols)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test, sens_train, sens_test = train_test_split(\n",
    "    X_processed_no_sensitive, y, sens, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train the logistic regression model\n",
    "lr = LogisticRegression(random_state=42,max_iter=10000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_naive = lr.predict(X_test)\n",
    "\n",
    "# Evaluate baseline performance metrics\n",
    "naive_accuracy = accuracy_score(y_test, y_pred_naive)\n",
    "f1_score_naive = f1_score(y_test, y_pred_naive)\n",
    "\n",
    "# Evaluate fairness metrics for the baseline model\n",
    "naive_dp_diff = demographic_parity_difference(y_test, y_pred_naive, sensitive_features=sens_test)\n",
    "naive_eo_diff = equalized_odds_difference(y_test, y_pred_naive, sensitive_features=sens_test)\n",
    "\n",
    "print(\"=== Naive Model Metrics ===\")\n",
    "print(\"Accuracy:\", naive_accuracy)\n",
    "print(\"F1 score:\",f1_score_naive) \n",
    "print(\"Demographic Parity Difference:\", naive_dp_diff)\n",
    "print(\"Equalized Odds Difference:\", naive_eo_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimum fairness search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define candidate methods for each stage.\n",
    "pre_methods = {\n",
    "    \"None\": fp.pre_none,\n",
    "    \"CorrRemover\": fp.pre_correlation_remover,\n",
    "    \"SensitiveResampling\": fp.pre_sensitive_resampling  # new candidate\n",
    "}\n",
    "\n",
    "in_methods = {\n",
    "    \"Baseline\": fp.in_baseline,\n",
    "    \"Reweighting\": fp.in_reweighting,\n",
    "    \"ExpGrad_DP\": fp.in_expgrad_dp,\n",
    "    \"ExpGrad_EO\": fp.in_expgrad_eo\n",
    "}\n",
    "\n",
    "post_methods = {\n",
    "    \"None\": fp.post_none,\n",
    "    \"Threshold_DP\": fp.post_threshold_dp,\n",
    "    \"Threshold_EO\": fp.post_threshold_eo\n",
    "}\n",
    "\n",
    "# Run experiments:\n",
    "results = fp.run_experiments(pre_methods, in_methods, post_methods,\n",
    "                             X_train, y_train, sens_train,\n",
    "                             X_test, y_test, sens_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only pareto optimal methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "objectives = {\"f1_score\": True, \"Demographic_parity\": False, \"Equalized_odds\": False}\n",
    "\n",
    "frontier = fp.pareto_frontier(results, objectives)\n",
    "\n",
    "print(\"Pareto Frontier configurations:\")\n",
    "for config, metrics in frontier.items():\n",
    "    print(f\"{config}: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply thresholds on biase and portion of retained accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set thresholds on accurcy, demographic parity and equalized odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_threshold = 0.4\n",
    "demographic_parity_threshold = 0.2\n",
    "equalized_odds_threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter results based on thresholds.\n",
    "filtered = fp.filter_results(frontier, f1_threshold=f1_threshold,\n",
    "                            dp_threshold=demographic_parity_threshold, eo_threshold=equalized_odds_threshold)\n",
    "\n",
    "print(\"\\nFiltered Results (satisfying thresholds):\")\n",
    "for config, metrics in filtered.items():\n",
    "    print(config, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
